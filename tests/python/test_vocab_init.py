#!/usr/bin/env python3
"""
æµ‹è¯•ä»dictç›®å½•åŠ è½½åˆå§‹åŒ–è¯è¡¨çš„åŠŸèƒ½
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

def test_load_vocab_from_dict():
    """æµ‹è¯•ä»dictç›®å½•åŠ è½½åˆå§‹åŒ–è¯è¡¨"""
    print("æµ‹è¯•ä»dictç›®å½•åŠ è½½åˆå§‹åŒ–è¯è¡¨...")
    
    try:
        import zero_tokenizer
        print("âœ“ zero_tokenizeræ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âœ— zero_tokenizeræ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # åˆ›å»ºBPEåˆ†è¯å™¨
    try:
        tokenizer = zero_tokenizer.Tokenizer()
        print("âœ“ BPEåˆ†è¯å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âœ— BPEåˆ†è¯å™¨åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # è·å–åˆå§‹è¯æ±‡è¡¨å¤§å°
    initial_vocab_size = tokenizer.get_vocab_size()
    print(f"åˆå§‹è¯æ±‡è¡¨å¤§å°: {initial_vocab_size}")
    
    # æµ‹è¯•åŠ è½½åŒ–å­¦åŸºæœ¬å…ƒç´ è¡¨
    try:
        tokenizer.load_vocab_from_dict("åŒ–å­¦åŸºæœ¬å…ƒç´ è¡¨.txt")
        print("âœ“ æˆåŠŸåŠ è½½åŒ–å­¦åŸºæœ¬å…ƒç´ è¡¨")
    except Exception as e:
        print(f"âœ— åŠ è½½åŒ–å­¦åŸºæœ¬å…ƒç´ è¡¨å¤±è´¥: {e}")
        return False
    
    # æ£€æŸ¥è¯æ±‡è¡¨æ˜¯å¦å¢åŠ 
    after_elements_vocab_size = tokenizer.get_vocab_size()
    print(f"åŠ è½½åŒ–å­¦å…ƒç´ åè¯æ±‡è¡¨å¤§å°: {after_elements_vocab_size}")
    
    if after_elements_vocab_size <= initial_vocab_size:
        print("âœ— è¯æ±‡è¡¨å¤§å°æ²¡æœ‰å¢åŠ ")
        return False
    
    # æµ‹è¯•ç¼–ç åŒ–å­¦å…ƒç´ 
    test_cases = [
        ("H", True),  # åº”è¯¥èƒ½ç¼–ç 
        ("æ°¢", True),  # åº”è¯¥èƒ½ç¼–ç 
        ("Li", True),  # åº”è¯¥èƒ½ç¼–ç 
        ("é”‚", True),  # åº”è¯¥èƒ½ç¼–ç 
        ("æ°¢Li", True),  # åº”è¯¥èƒ½ç¼–ç 
        ("æœªçŸ¥å…ƒç´ ", False),  # å¯èƒ½æ— æ³•ç¼–ç 
    ]
    
    print("\nç¼–ç æµ‹è¯•:")
    for text, should_succeed in test_cases:
        try:
            tokens = tokenizer.encode(text)
            if should_succeed:
                print(f"âœ“ æ–‡æœ¬ '{text}' ç¼–ç æˆåŠŸ: {tokens}")
            else:
                print(f"? æ–‡æœ¬ '{text}' ç¼–ç æˆåŠŸï¼ˆå¯èƒ½å¤±è´¥ï¼‰: {tokens}")
        except Exception as e:
            if should_succeed:
                print(f"âœ— æ–‡æœ¬ '{text}' ç¼–ç å¤±è´¥: {e}")
                return False
            else:
                print(f"? æ–‡æœ¬ '{text}' ç¼–ç å¤±è´¥ï¼ˆé¢„æœŸï¼‰: {e}")
    
    # æµ‹è¯•åŠ è½½å¸¸ç”¨æ±‰å­—å­—è¡¨
    try:
        tokenizer.load_vocab_from_dict("å¸¸ç”¨æ±‰å­—å­—è¡¨.txt")
        print("\nâœ“ æˆåŠŸåŠ è½½å¸¸ç”¨æ±‰å­—å­—è¡¨")
    except Exception as e:
        print(f"\nâœ— åŠ è½½å¸¸ç”¨æ±‰å­—å­—è¡¨å¤±è´¥: {e}")
        return False
    
    # æ£€æŸ¥è¯æ±‡è¡¨æ˜¯å¦è¿›ä¸€æ­¥å¢åŠ 
    final_vocab_size = tokenizer.get_vocab_size()
    print(f"åŠ è½½å¸¸ç”¨æ±‰å­—åè¯æ±‡è¡¨å¤§å°: {final_vocab_size}")
    
    if final_vocab_size <= after_elements_vocab_size:
        print("âœ— è¯æ±‡è¡¨å¤§å°æ²¡æœ‰å¢åŠ ")
        return False
    
    # æµ‹è¯•ç¼–ç ä¸­æ–‡æ–‡æœ¬
    chinese_test_cases = [
        ("ä½ ", True),  # åº”è¯¥èƒ½ç¼–ç 
        ("å¥½", True),  # åº”è¯¥èƒ½ç¼–ç 
        ("ä½ å¥½", True),  # åº”è¯¥èƒ½ç¼–ç 
        ("ä¸–ç•Œ", True),  # åº”è¯¥èƒ½ç¼–ç 
        ("ä½ å¥½ä¸–ç•Œ", True),  # åº”è¯¥èƒ½ç¼–ç 
    ]
    
    print("\nä¸­æ–‡ç¼–ç æµ‹è¯•:")
    for text, should_succeed in chinese_test_cases:
        try:
            tokens = tokenizer.encode(text)
            if should_succeed:
                print(f"âœ“ æ–‡æœ¬ '{text}' ç¼–ç æˆåŠŸ: {tokens}")
            else:
                print(f"? æ–‡æœ¬ '{text}' ç¼–ç æˆåŠŸï¼ˆå¯èƒ½å¤±è´¥ï¼‰: {tokens}")
        except Exception as e:
            if should_succeed:
                print(f"âœ— æ–‡æœ¬ '{text}' ç¼–ç å¤±è´¥: {e}")
                return False
            else:
                print(f"? æ–‡æœ¬ '{text}' ç¼–ç å¤±è´¥ï¼ˆé¢„æœŸï¼‰: {e}")
    
    # æµ‹è¯•ç¼–ç è§£ç å¾€è¿”
    print("\nç¼–ç è§£ç å¾€è¿”æµ‹è¯•:")
    test_texts = ["æ°¢", "Li", "æ°¢Li", "ä½ ", "å¥½", "ä½ å¥½"]
    for text in test_texts:
        try:
            tokens = tokenizer.encode(text)
            decoded = tokenizer.decode(tokens)
            if decoded == text:
                print(f"âœ“ '{text}' -> {tokens} -> '{decoded}' (å¾€è¿”æˆåŠŸ)")
            else:
                print(f"âœ— '{text}' -> {tokens} -> '{decoded}' (å¾€è¿”å¤±è´¥)")
                return False
        except Exception as e:
            print(f"âœ— '{text}' å¾€è¿”æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    print("\nâœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    return True

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹æµ‹è¯•åˆå§‹åŒ–è¯è¡¨åŠŸèƒ½...")
    print("=" * 50)
    
    if test_load_vocab_from_dict():
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("\n" + "=" * 50)
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        return 1

if __name__ == "__main__":
    sys.exit(main())