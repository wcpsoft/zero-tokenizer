#!/usr/bin/env python3
"""
ç®€å•çš„Pythonæµ‹è¯•ï¼ŒéªŒè¯zero_tokenizeræ¨¡å—æ˜¯å¦å¯ä»¥æ­£ç¡®å¯¼å…¥å’Œä½¿ç”¨
"""

import sys

def test_import():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    try:
        import zero_tokenizer
        print("âœ“ zero_tokenizeræ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
    except ImportError as e:
        print(f"âœ— zero_tokenizeræ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_tokenizer_classes():
    """æµ‹è¯•åˆ†è¯å™¨ç±»æ˜¯å¦å¯ç”¨"""
    try:
        from zero_tokenizer import Tokenizer, BBPETokenizer, UnigramTokenizer, WordPieceTokenizer
        print("âœ“ æ‰€æœ‰åˆ†è¯å™¨ç±»å¯¼å…¥æˆåŠŸ")
        return True
    except ImportError as e:
        print(f"âœ— åˆ†è¯å™¨ç±»å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_tokenizer_creation():
    """æµ‹è¯•åˆ†è¯å™¨å®ä¾‹åˆ›å»º"""
    try:
        from zero_tokenizer import Tokenizer
        tokenizer = Tokenizer()
        print("âœ“ Tokenizerå®ä¾‹åˆ›å»ºæˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— Tokenizerå®ä¾‹åˆ›å»ºå¤±è´¥: {e}")
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹è¿è¡Œzero_tokenizeråŸºæœ¬æµ‹è¯•...")
    print("=" * 50)
    
    tests = [
        test_import,
        test_tokenizer_classes,
        test_tokenizer_creation,
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
        print()
    
    print("=" * 50)
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        return 1

if __name__ == "__main__":
    sys.exit(main())