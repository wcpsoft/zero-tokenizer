"""æµ‹è¯•æ‰¹é‡ç¼–ç è§£ç åŠŸèƒ½

Python 3.10-3.12 å…¼å®¹æ€§æµ‹è¯•
"""

import sys
import time
import pytest


def test_bbpe_encode_batch():
    """æµ‹è¯•BBPEæ‰¹é‡ç¼–ç åŠŸèƒ½"""
    from zero_tokenizer import BBPETokenizer

    tokenizer = BBPETokenizer()
    texts = [
        "Hello world!",
        "ä½ å¥½ä¸–ç•Œï¼",
        "Test text",
        "Another test",
    ]

    tokenizer.train(texts, 300)

    # æ‰¹é‡ç¼–ç 
    batch_tokens = tokenizer.encode_batch(texts)
    assert len(batch_tokens) == len(texts)

    # éªŒè¯æ¯ä¸ªç»“æœ
    for i, tokens in enumerate(batch_tokens):
        decoded = tokenizer.decode(tokens)
        assert decoded == texts[i], f"è§£ç ä¸ä¸€è‡´: {texts[i]} != {decoded}"


def test_bbpe_decode_batch():
    """æµ‹è¯•BBPEæ‰¹é‡è§£ç åŠŸèƒ½"""
    from zero_tokenizer import BBPETokenizer

    tokenizer = BBPETokenizer()
    texts = ["Hello", "World", "Test"]

    tokenizer.train(texts, 300)

    # ç¼–ç 
    token_lists = [tokenizer.encode(text) for text in texts]

    # æ‰¹é‡è§£ç 
    decoded_texts = tokenizer.decode_batch(token_lists)
    assert decoded_texts == texts


def test_large_batch():
    """æµ‹è¯•å¤§æ‰¹é‡å¤„ç†"""
    from zero_tokenizer import BBPETokenizer

    tokenizer = BBPETokenizer()

    # 100ä¸ªæ–‡æœ¬
    texts = [f"Test text number {i}" for i in range(100)]
    tokenizer.train(texts[:10], 300)

    # æ‰¹é‡ç¼–ç åº”è¯¥å¿«é€Ÿå®Œæˆ
    start = time.time()
    batch_tokens = tokenizer.encode_batch(texts)
    duration = time.time() - start

    assert len(batch_tokens) == 100
    print(f"æ‰¹é‡ç¼–ç 100ä¸ªæ–‡æœ¬è€—æ—¶: {duration:.3f}ç§’")

    # éªŒè¯æ€§èƒ½ï¼ˆåº”è¯¥<1ç§’ï¼‰
    assert duration < 5.0, f"æ‰¹é‡ç¼–ç å¤ªæ…¢: {duration}ç§’"


def test_unigram_encode_batch():
    """æµ‹è¯•Unigramæ‰¹é‡ç¼–ç """
    from zero_tokenizer import UnigramTokenizer

    tokenizer = UnigramTokenizer()
    texts = [
        "æµ‹è¯•æ–‡æœ¬1",
        "æµ‹è¯•æ–‡æœ¬2",
        "æµ‹è¯•æ–‡æœ¬3",
    ]

    tokenizer.train(texts, 16000)

    batch_tokens = tokenizer.encode_batch(texts)
    assert len(batch_tokens) == len(texts)

    # éªŒè¯è§£ç ä¸€è‡´æ€§
    for i, tokens in enumerate(batch_tokens):
        decoded = tokenizer.decode(tokens)
        assert decoded == texts[i]


def test_wordpiece_encode_batch():
    """æµ‹è¯•WordPieceæ‰¹é‡ç¼–ç """
    from zero_tokenizer import WordPieceTokenizer

    tokenizer = WordPieceTokenizer()
    texts = [
        "æµ‹è¯•æ–‡æœ¬1",
        "æµ‹è¯•æ–‡æœ¬2",
        "æµ‹è¯•æ–‡æœ¬3",
    ]

    tokenizer.train(texts, 16000)

    batch_tokens = tokenizer.encode_batch(texts)
    assert len(batch_tokens) == len(texts)


def test_bpe_encode_batch():
    """æµ‹è¯•BPEæ‰¹é‡ç¼–ç """
    from zero_tokenizer import Tokenizer

    tokenizer = Tokenizer()
    texts = [
        "Hello world",
        "Test text",
        "Another example",
    ]

    tokenizer.train_from_iterator(texts, 300)

    # BPEä½¿ç”¨ä¸åŒçš„æ–¹æ³•å
    batch_tokens = [tokenizer.py_encode(text) for text in texts]
    assert len(batch_tokens) == len(texts)

    # éªŒè¯è§£ç 
    for i, tokens in enumerate(batch_tokens):
        decoded = tokenizer.py_decode(tokens)
        assert decoded == texts[i]


def test_empty_texts_in_batch():
    """æµ‹è¯•æ‰¹é‡å¤„ç†ä¸­çš„ç©ºå­—ç¬¦ä¸²"""
    from zero_tokenizer import BBPETokenizer

    tokenizer = BBPETokenizer()
    tokenizer.train(["test"], 300)

    # åŒ…å«ç©ºå­—ç¬¦ä¸²çš„æ‰¹æ¬¡
    texts = ["Hello", "", "World", ""]

    batch_tokens = tokenizer.encode_batch(texts)

    # éªŒè¯ç©ºå­—ç¬¦ä¸²ç¼–ç ä¸ºç©ºåˆ—è¡¨
    assert len(batch_tokens[0]) > 0  # "Hello"
    assert len(batch_tokens[1]) == 0  # ""
    assert len(batch_tokens[2]) > 0  # "World"
    assert len(batch_tokens[3]) == 0  # ""


def test_mixed_length_batch():
    """æµ‹è¯•ä¸åŒé•¿åº¦æ–‡æœ¬çš„æ‰¹é‡å¤„ç†"""
    from zero_tokenizer import BBPETokenizer

    tokenizer = BBPETokenizer()

    # ä¸åŒé•¿åº¦çš„æ–‡æœ¬
    texts = [
        "a",
        "ab",
        "abc def",
        "This is a much longer text for testing batch operations",
    ]

    tokenizer.train(texts, 300)

    batch_tokens = tokenizer.encode_batch(texts)

    # éªŒè¯æ¯ä¸ªç»“æœ
    for i, tokens in enumerate(batch_tokens):
        decoded = tokenizer.decode(tokens)
        assert decoded == texts[i]


def test_batch_with_unicode():
    """æµ‹è¯•åŒ…å«Unicodeå­—ç¬¦çš„æ‰¹é‡å¤„ç†"""
    from zero_tokenizer import BBPETokenizer

    tokenizer = BBPETokenizer()
    texts = [
        "Hello ğŸ‘‹",
        "ä¸–ç•Œ ğŸŒ",
        "Testing emoji ğŸ‰",
        "Mixed ä¸­è‹±æ–‡ text",
    ]

    tokenizer.train(texts, 300)

    batch_tokens = tokenizer.encode_batch(texts)

    for i, tokens in enumerate(batch_tokens):
        decoded = tokenizer.decode(tokens)
        assert decoded == texts[i]


def test_batch_performance_comparison():
    """æ¯”è¾ƒæ‰¹é‡å¤„ç†å’Œå•ç‹¬å¤„ç†çš„æ€§èƒ½"""
    from zero_tokenizer import BBPETokenizer

    tokenizer = BBPETokenizer()
    texts = [f"Test text {i}" for i in range(50)]
    tokenizer.train(texts[:5], 300)

    # å•ç‹¬å¤„ç†
    start = time.time()
    individual_tokens = [tokenizer.encode(text) for text in texts]
    individual_time = time.time() - start

    # æ‰¹é‡å¤„ç†
    start = time.time()
    batch_tokens = tokenizer.encode_batch(texts)
    batch_time = time.time() - start

    print(f"å•ç‹¬å¤„ç†: {individual_time:.3f}ç§’")
    print(f"æ‰¹é‡å¤„ç†: {batch_time:.3f}ç§’")

    # éªŒè¯ç»“æœä¸€è‡´
    assert len(individual_tokens) == len(batch_tokens)
    for i in range(len(texts)):
        assert individual_tokens[i] == batch_tokens[i]


if __name__ == "__main__":
    # æ”¯æŒç›´æ¥è¿è¡Œ
    pytest.main([__file__, "-v"])
