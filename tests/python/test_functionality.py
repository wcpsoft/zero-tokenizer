"""
æµ‹è¯•åˆ†è¯å™¨çš„ç¼–ç å’Œè§£ç åŠŸèƒ½
"""
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

def test_bpe_tokenizer():
    """æµ‹è¯•BPEåˆ†è¯å™¨çš„åŸºæœ¬åŠŸèƒ½"""
    try:
        from zero_tokenizer import Tokenizer
        
        # åˆ›å»ºåˆ†è¯å™¨å®ä¾‹
        tokenizer = Tokenizer()
        
        # æµ‹è¯•ç¼–ç 
        text = "Hello, world! ä½ å¥½ï¼Œä¸–ç•Œï¼"
        tokens = tokenizer.py_encode(text)
        print(f"âœ“ BPEç¼–ç æˆåŠŸ: {text} -> {tokens}")
        
        # æµ‹è¯•è§£ç 
        decoded = tokenizer.py_decode(tokens)
        print(f"âœ“ BPEè§£ç æˆåŠŸ: {tokens} -> {decoded}")
        
        # æ£€æŸ¥ç¼–ç è§£ç æ˜¯å¦ä¸€è‡´
        if text == decoded:
            print("âœ“ BPEç¼–ç è§£ç ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡")
            return True
        else:
            print(f"âœ— BPEç¼–ç è§£ç ä¸ä¸€è‡´: åŸæ–‡='{text}' è§£ç ='{decoded}'")
            return False
            
    except Exception as e:
        print(f"âœ— BPEåˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_bbp_e_tokenizer():
    """æµ‹è¯•BBPEåˆ†è¯å™¨çš„åŸºæœ¬åŠŸèƒ½"""
    try:
        from zero_tokenizer import BBPETokenizer
        
        # åˆ›å»ºåˆ†è¯å™¨å®ä¾‹
        tokenizer = BBPETokenizer()
        
        # æµ‹è¯•ç¼–ç 
        text = "Hello, world! ä½ å¥½ï¼Œä¸–ç•Œï¼"
        tokens = tokenizer.encode(text)
        print(f"âœ“ BBPEç¼–ç æˆåŠŸ: {text} -> {tokens}")
        
        # æµ‹è¯•è§£ç 
        decoded = tokenizer.decode(tokens)
        print(f"âœ“ BBPEè§£ç æˆåŠŸ: {tokens} -> {decoded}")
        
        # æ£€æŸ¥ç¼–ç è§£ç æ˜¯å¦ä¸€è‡´
        if text == decoded:
            print("âœ“ BBPEç¼–ç è§£ç ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡")
            return True
        else:
            print(f"âœ— BBPEç¼–ç è§£ç ä¸ä¸€è‡´: åŸæ–‡='{text}' è§£ç ='{decoded}'")
            return False
            
    except Exception as e:
        print(f"âœ— BBPEåˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_unigram_tokenizer():
    """æµ‹è¯•Unigramåˆ†è¯å™¨çš„åŸºæœ¬åŠŸèƒ½"""
    try:
        from zero_tokenizer import UnigramTokenizer
        
        # åˆ›å»ºåˆ†è¯å™¨å®ä¾‹
        tokenizer = UnigramTokenizer()
        
        # æµ‹è¯•ç¼–ç 
        text = "Hello, world! ä½ å¥½ï¼Œä¸–ç•Œï¼"
        tokens = tokenizer.encode(text)
        print(f"âœ“ Unigramç¼–ç æˆåŠŸ: {text} -> {tokens}")
        
        # æµ‹è¯•è§£ç 
        decoded = tokenizer.decode(tokens)
        print(f"âœ“ Unigramè§£ç æˆåŠŸ: {tokens} -> {decoded}")
        
        # æ£€æŸ¥ç¼–ç è§£ç æ˜¯å¦ä¸€è‡´
        if text == decoded:
            print("âœ“ Unigramç¼–ç è§£ç ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡")
            return True
        else:
            print(f"âœ— Unigramç¼–ç è§£ç ä¸ä¸€è‡´: åŸæ–‡='{text}' è§£ç ='{decoded}'")
            return False
            
    except Exception as e:
        print(f"âœ— Unigramåˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_wordpiece_tokenizer():
    """æµ‹è¯•WordPieceåˆ†è¯å™¨çš„åŸºæœ¬åŠŸèƒ½"""
    try:
        from zero_tokenizer import WordPieceTokenizer
        
        # åˆ›å»ºåˆ†è¯å™¨å®ä¾‹
        tokenizer = WordPieceTokenizer()
        
        # æµ‹è¯•ç¼–ç 
        text = "Hello, world! ä½ å¥½ï¼Œä¸–ç•Œï¼"
        tokens = tokenizer.encode(text)
        print(f"âœ“ WordPieceç¼–ç æˆåŠŸ: {text} -> {tokens}")
        
        # æµ‹è¯•è§£ç 
        decoded = tokenizer.decode(tokens)
        print(f"âœ“ WordPieceè§£ç æˆåŠŸ: {tokens} -> {decoded}")
        
        # æ£€æŸ¥ç¼–ç è§£ç æ˜¯å¦ä¸€è‡´
        if text == decoded:
            print("âœ“ WordPieceç¼–ç è§£ç ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡")
            return True
        else:
            print(f"âœ— WordPieceç¼–ç è§£ç ä¸ä¸€è‡´: åŸæ–‡='{text}' è§£ç ='{decoded}'")
            return False
            
    except Exception as e:
        print(f"âœ— WordPieceåˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹è¿è¡Œåˆ†è¯å™¨åŠŸèƒ½æµ‹è¯•...")
    print("=" * 50)
    
    tests = [
        test_bpe_tokenizer,
        test_bbp_e_tokenizer,
        test_unigram_tokenizer,
        test_wordpiece_tokenizer
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
        print()  # ç©ºè¡Œåˆ†éš”
    
    print("=" * 50)
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        return 1

if __name__ == "__main__":
    sys.exit(main())