//! Unicodeå’Œç‰¹æ®Šå­—ç¬¦æµ‹è¯•
//!
//! æµ‹è¯•å„ç§Unicodeå­—ç¬¦çš„ç¼–ç è§£ç ï¼ŒåŒ…æ‹¬emojiã€ç»„åˆå­—ç¬¦ã€RTLæ–‡å­—ç­‰

use zero_tokenizer::prelude::*;

#[test]
fn test_emoji_encoding() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();
    let text = "Hello ğŸ‘‹ World ğŸŒ Test ğŸ‰";

    tokenizer.train(vec![text.to_string()], 300).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_multiple_emojis() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();
    let text = "ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ¤£ğŸ˜‚ğŸ™‚ğŸ™ƒğŸ˜‰ğŸ˜ŠğŸ˜‡ğŸ¥°ğŸ˜ğŸ¤©ğŸ˜˜";

    tokenizer.train(vec![text.to_string()], 300).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_combining_characters() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();

    // ä½¿ç”¨ç»„åˆå­—ç¬¦: Ã© å¯ä»¥æ˜¯ e + Â´ (combining acute accent)
    let text = "cafÃ© naÃ¯ve";
    tokenizer.train(vec![text.to_string()], 300).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_zero_width_characters() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();

    // é›¶å®½å­—ç¬¦
    let text = "Hello\u{200B}World\u{FEFF}Test"; // é›¶å®½ç©ºæ ¼å’Œé›¶å®½éæ–­ç©ºæ ¼
    tokenizer.train(vec![text.to_string()], 300).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_rtl_text_arabic() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();

    // é˜¿æ‹‰ä¼¯æ–‡ï¼ˆä»å³åˆ°å·¦ï¼‰
    let text = "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…";
    tokenizer.train(vec![text.to_string()], 300).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_rtl_text_hebrew() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();

    // å¸Œä¼¯æ¥æ–‡ï¼ˆä»å³åˆ°å·¦ï¼‰
    let text = "×©×œ×•× ×¢×•×œ×";
    tokenizer.train(vec![text.to_string()], 300).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_mixed_scripts() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();

    // æ··åˆå¤šç§æ–‡å­—
    let text = "Hello ä½ å¥½ Ù…Ø±Ø­Ø¨Ø§ ã“ã‚“ã«ã¡ã¯ ì•ˆë…•í•˜ì„¸ìš”";
    tokenizer.train(vec![text.to_string()], 500).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_control_characters() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();

    // æ§åˆ¶å­—ç¬¦
    let text = "Hello\nWorld\tTest\r\nDone";
    tokenizer.train(vec![text.to_string()], 300).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_surrogate_pairs() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();

    // Unicodeä»£ç†å¯¹ï¼ˆ4å­—èŠ‚å­—ç¬¦ï¼‰- æ•°å­¦å­—æ¯
    let text = "ğ•³ğ–Šğ–‘ğ–‘ğ–” ğ–‚ğ–”ğ–—ğ–‘ğ–‰";
    tokenizer.train(vec![text.to_string()], 300).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_cjk_unified_ideographs() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();

    // CJKç»Ÿä¸€æ±‰å­—
    let text = "ä¸­æ–‡æµ‹è¯• æ—¥æœ¬èªãƒ†ã‚¹ãƒˆ í•œêµ­ì–´í…ŒìŠ¤íŠ¸";
    tokenizer.train(vec![text.to_string()], 400).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_special_punctuation() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();

    // ç‰¹æ®Šæ ‡ç‚¹ç¬¦å· (ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²)
    let text = r#"â€¹â€ºÂ«Â»â€šâ€''""â€¦â€“â€”"#; // å„ç§å¼•å·ã€çœç•¥å·ã€ç ´æŠ˜å·
    tokenizer.train(vec![text.to_string()], 300).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_mathematical_symbols() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();

    // æ•°å­¦ç¬¦å·
    let text = "âˆ‘âˆ«âˆšâˆâ‰ â‰¤â‰¥Â±Ã—Ã·";
    tokenizer.train(vec![text.to_string()], 300).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_currency_symbols() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();

    // è´§å¸ç¬¦å·
    let text = "$â‚¬Â£Â¥â‚¹â‚½â‚©Â¢";
    tokenizer.train(vec![text.to_string()], 300).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_emoji_with_skin_tone() {
    let mut tokenizer = zero_tokenizer::prelude::bbpe().unwrap();

    // å¸¦è‚¤è‰²ä¿®é¥°ç¬¦çš„emoji
    let text = "ğŸ‘‹ğŸ»ğŸ‘‹ğŸ¼ğŸ‘‹ğŸ½ğŸ‘‹ğŸ¾ğŸ‘‹ğŸ¿";
    tokenizer.train(vec![text.to_string()], 300).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_unigram_unicode() {
    let mut tokenizer = zero_tokenizer::prelude::unigram().unwrap();

    // Unigramå¯¹å„ç§Unicodeçš„æ”¯æŒ
    let text = "æµ‹è¯•emojiğŸ‘‹å’Œç‰¹æ®Šå­—ç¬¦â‚¬Â£Â¥";
    tokenizer.train(vec![text.to_string()], 16000).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}

#[test]
fn test_wordpiece_unicode() {
    let mut tokenizer = zero_tokenizer::prelude::wordpiece().unwrap();

    // WordPieceå¯¹å„ç§Unicodeçš„æ”¯æŒ
    let text = "æµ‹è¯•emojiğŸ‰å’Œæ•°å­¦ç¬¦å·âˆ‘âˆ«";
    tokenizer.train(vec![text.to_string()], 16000).unwrap();

    let tokens = tokenizer.encode(text).unwrap();
    let decoded = tokenizer.decode(&tokens).unwrap();

    assert_eq!(text, decoded);
}
