#!/usr/bin/env python3
"""
Unigram åˆ†è¯å™¨ä½¿ç”¨ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é¢„è®­ç»ƒçš„Unigramåˆ†è¯å™¨è¿›è¡Œæ–‡æœ¬ç¼–ç å’Œè§£ç ã€‚
Unigramè¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§åŸºäºæ¦‚ç‡çš„åˆ†è¯æ–¹æ³•ï¼Œå®ƒä¸ºæ¯ä¸ªå­è¯åˆ†é…ä¸€ä¸ªæ¦‚ç‡ã€‚
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    import zero_tokenizer
except ImportError:
    print("é”™è¯¯: æ— æ³•å¯¼å…¥zero_tokenizeråº“ã€‚è¯·ç¡®ä¿å·²å®‰è£…Pythonç»‘å®šã€‚")
    print("å¯ä»¥å°è¯•è¿è¡Œ: pip install -e .")
    sys.exit(1)


def main():
    print("Unigramåˆ†è¯å™¨ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºUnigramåˆ†è¯å™¨å®ä¾‹
    print("\n1. åˆ›å»ºUnigramåˆ†è¯å™¨å®ä¾‹")
    tokenizer = zero_tokenizer.unigram()
    print("âœ“ Unigramåˆ†è¯å™¨åˆ›å»ºæˆåŠŸ")
    
    # ç¤ºä¾‹æ–‡æœ¬
    text = "è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•Unigramåˆ†è¯å™¨çš„ç¤ºä¾‹æ–‡æœ¬ã€‚Hello, world! ğŸŒ"
    print(f"\n2. åŸå§‹æ–‡æœ¬: {text}")
    
    # ç¼–ç æ–‡æœ¬
    print("\n3. ç¼–ç æ–‡æœ¬...")
    tokens = tokenizer.encode(text)
    print(f"âœ“ ç¼–ç æˆåŠŸï¼Œå¾—åˆ° {len(tokens)} ä¸ªtoken")
    print(f"Token IDs: {tokens}")
    
    # è§£ç æ–‡æœ¬
    print("\n4. è§£ç token...")
    decoded_text = tokenizer.decode(tokens)
    print(f"âœ“ è§£ç æˆåŠŸ")
    print(f"è§£ç æ–‡æœ¬: {decoded_text}")
    
    # éªŒè¯ç¼–ç è§£ç çš„ä¸€è‡´æ€§
    print("\n5. éªŒè¯ç¼–ç è§£ç ä¸€è‡´æ€§...")
    if text == decoded_text:
        print("âœ“ ç¼–ç è§£ç ä¸€è‡´ï¼Œæ— æŸè½¬æ¢")
    else:
        print("âœ— ç¼–ç è§£ç ä¸ä¸€è‡´ï¼Œå¯èƒ½å­˜åœ¨ä¿¡æ¯æŸå¤±")
        print(f"åŸå§‹: {text}")
        print(f"è§£ç : {decoded_text}")
    
    # å±•ç¤ºtokenåˆ°å•è¯çš„æ˜ å°„
    print("\n6. Tokenåˆ°å•è¯çš„æ˜ å°„:")
    for i, token_id in enumerate(tokens[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ªtoken
        token = tokenizer.id_to_token(token_id)
        score = tokenizer.get_score(token_id) if hasattr(tokenizer, 'get_score') else None
        if score is not None:
            print(f"  Token {i+1}: ID={token_id}, Token='{token}', Score={score:.4f}")
        else:
            print(f"  Token {i+1}: ID={token_id}, Token='{token}'")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n7. åˆ†è¯å™¨ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}")
    print(f"  ç‰¹æ®Štokenæ•°é‡: {len(tokenizer.get_special_tokens())}")
    
    # Unigramåˆ†è¯å™¨çš„ç‰¹ç‚¹æ˜¯å¯ä»¥è·å–tokençš„æ¦‚ç‡åˆ†æ•°
    print("\n8. Unigramåˆ†è¯å™¨ç‰¹ç‚¹æµ‹è¯•:")
    test_words = ["hello", "world", "æµ‹è¯•", "åˆ†è¯"]
    print("Tokenæ¦‚ç‡åˆ†æ•°ç¤ºä¾‹:")
    for word in test_words:
        word_tokens = tokenizer.encode(word)
        if word_tokens:
            token_id = word_tokens[0]
            token = tokenizer.id_to_token(token_id)
            score = tokenizer.get_score(token_id) if hasattr(tokenizer, 'get_score') else None
            if score is not None:
                print(f"  '{token}': {score:.4f}")
            else:
                print(f"  '{token}': åˆ†æ•°ä¸å¯ç”¨")
    
    print("\nç¤ºä¾‹å®Œæˆ!")


if __name__ == "__main__":
    main()