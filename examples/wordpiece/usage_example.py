#!/usr/bin/env python3
"""
WordPiece åˆ†è¯å™¨ä½¿ç”¨ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é¢„è®­ç»ƒçš„WordPieceåˆ†è¯å™¨è¿›è¡Œæ–‡æœ¬ç¼–ç å’Œè§£ç ã€‚
WordPieceæ˜¯ä¸€ç§åŸºäºå­è¯çš„åˆ†è¯æ–¹æ³•ï¼Œå®ƒé€šè¿‡æœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„ä¼¼ç„¶å‡½æ•°æ¥å­¦ä¹ è¯æ±‡è¡¨ã€‚
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    import zero_tokenizer
except ImportError:
    print("é”™è¯¯: æ— æ³•å¯¼å…¥zero_tokenizeråº“ã€‚è¯·ç¡®ä¿å·²å®‰è£…Pythonç»‘å®šã€‚")
    print("å¯ä»¥å°è¯•è¿è¡Œ: pip install -e .")
    sys.exit(1)


def main():
    print("WordPieceåˆ†è¯å™¨ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºWordPieceåˆ†è¯å™¨å®ä¾‹
    print("\n1. åˆ›å»ºWordPieceåˆ†è¯å™¨å®ä¾‹")
    tokenizer = zero_tokenizer.wordpiece()
    print("âœ“ WordPieceåˆ†è¯å™¨åˆ›å»ºæˆåŠŸ")
    
    # ç¤ºä¾‹æ–‡æœ¬
    text = "è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•WordPieceåˆ†è¯å™¨çš„ç¤ºä¾‹æ–‡æœ¬ã€‚Hello, world! ğŸŒ"
    print(f"\n2. åŸå§‹æ–‡æœ¬: {text}")
    
    # ç¼–ç æ–‡æœ¬
    print("\n3. ç¼–ç æ–‡æœ¬...")
    tokens = tokenizer.encode(text)
    print(f"âœ“ ç¼–ç æˆåŠŸï¼Œå¾—åˆ° {len(tokens)} ä¸ªtoken")
    print(f"Token IDs: {tokens}")
    
    # è§£ç æ–‡æœ¬
    print("\n4. è§£ç token...")
    decoded_text = tokenizer.decode(tokens)
    print(f"âœ“ è§£ç æˆåŠŸ")
    print(f"è§£ç æ–‡æœ¬: {decoded_text}")
    
    # éªŒè¯ç¼–ç è§£ç çš„ä¸€è‡´æ€§
    print("\n5. éªŒè¯ç¼–ç è§£ç ä¸€è‡´æ€§...")
    if text == decoded_text:
        print("âœ“ ç¼–ç è§£ç ä¸€è‡´ï¼Œæ— æŸè½¬æ¢")
    else:
        print("âœ— ç¼–ç è§£ç ä¸ä¸€è‡´ï¼Œå¯èƒ½å­˜åœ¨ä¿¡æ¯æŸå¤±")
        print(f"åŸå§‹: {text}")
        print(f"è§£ç : {decoded_text}")
    
    # å±•ç¤ºtokenåˆ°å•è¯çš„æ˜ å°„
    print("\n6. Tokenåˆ°å•è¯çš„æ˜ å°„:")
    for i, token_id in enumerate(tokens[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ªtoken
        token = tokenizer.id_to_token(token_id)
        print(f"  Token {i+1}: ID={token_id}, Token='{token}'")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n7. åˆ†è¯å™¨ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}")
    print(f"  ç‰¹æ®Štokenæ•°é‡: {len(tokenizer.get_special_tokens())}")
    
    # WordPieceçš„ç‰¹ç‚¹æ˜¯ä½¿ç”¨##è¡¨ç¤ºå­è¯çš„å»¶ç»­
    print("\n8. WordPieceç‰¹ç‚¹æµ‹è¯•:")
    test_words = ["unhappiness", "preprocessing", "tokenization", "unbelievable"]
    print("å­è¯åˆ†å‰²ç¤ºä¾‹:")
    for word in test_words:
        word_tokens = tokenizer.encode(word)
        word_str_tokens = [tokenizer.id_to_token(t) for t in word_tokens]
        print(f"  '{word}': {' '.join(word_str_tokens)}")
    
    print("\nç¤ºä¾‹å®Œæˆ!")


if __name__ == "__main__":
    main()