#!/usr/bin/env python3
"""
BBPE (Byte-Level BPE) åˆ†è¯å™¨ä½¿ç”¨ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é¢„è®­ç»ƒçš„BBPEåˆ†è¯å™¨è¿›è¡Œæ–‡æœ¬ç¼–ç å’Œè§£ç ã€‚
BBPEæ˜¯BPEçš„ä¸€ç§å˜ä½“ï¼Œå®ƒåœ¨å­—èŠ‚çº§åˆ«ä¸Šè¿›è¡Œæ“ä½œï¼Œå¯ä»¥å¤„ç†ä»»æ„Unicodeå­—ç¬¦ã€‚
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    import zero_tokenizer
except ImportError:
    print("é”™è¯¯: æ— æ³•å¯¼å…¥zero_tokenizeråº“ã€‚è¯·ç¡®ä¿å·²å®‰è£…Pythonç»‘å®šã€‚")
    print("å¯ä»¥å°è¯•è¿è¡Œ: pip install -e .")
    sys.exit(1)


def main():
    print("BBPEåˆ†è¯å™¨ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºBBPEåˆ†è¯å™¨å®ä¾‹
    print("\n1. åˆ›å»ºBBPEåˆ†è¯å™¨å®ä¾‹")
    tokenizer = zero_tokenizer.bbpe()
    print("âœ“ BBPEåˆ†è¯å™¨åˆ›å»ºæˆåŠŸ")
    
    # ç¤ºä¾‹æ–‡æœ¬ï¼ŒåŒ…å«å„ç§Unicodeå­—ç¬¦
    text = "è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•BBPEåˆ†è¯å™¨çš„ç¤ºä¾‹æ–‡æœ¬ã€‚Hello, world! ğŸŒ"
    print(f"\n2. åŸå§‹æ–‡æœ¬: {text}")
    
    # ç¼–ç æ–‡æœ¬
    print("\n3. ç¼–ç æ–‡æœ¬...")
    tokens = tokenizer.encode(text)
    print(f"âœ“ ç¼–ç æˆåŠŸï¼Œå¾—åˆ° {len(tokens)} ä¸ªtoken")
    print(f"Token IDs: {tokens}")
    
    # è§£ç æ–‡æœ¬
    print("\n4. è§£ç token...")
    decoded_text = tokenizer.decode(tokens)
    print(f"âœ“ è§£ç æˆåŠŸ")
    print(f"è§£ç æ–‡æœ¬: {decoded_text}")
    
    # éªŒè¯ç¼–ç è§£ç çš„ä¸€è‡´æ€§
    print("\n5. éªŒè¯ç¼–ç è§£ç ä¸€è‡´æ€§...")
    if text == decoded_text:
        print("âœ“ ç¼–ç è§£ç ä¸€è‡´ï¼Œæ— æŸè½¬æ¢")
    else:
        print("âœ— ç¼–ç è§£ç ä¸ä¸€è‡´ï¼Œå¯èƒ½å­˜åœ¨ä¿¡æ¯æŸå¤±")
        print(f"åŸå§‹: {text}")
        print(f"è§£ç : {decoded_text}")
    
    # å±•ç¤ºtokenåˆ°å•è¯çš„æ˜ å°„
    print("\n6. Tokenåˆ°å•è¯çš„æ˜ å°„:")
    for i, token_id in enumerate(tokens[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ªtoken
        token = tokenizer.id_to_token(token_id)
        print(f"  Token {i+1}: ID={token_id}, Token='{token}'")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n7. åˆ†è¯å™¨ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {tokenizer.get_vocab_size()}")
    print(f"  ç‰¹æ®Štokenæ•°é‡: {len(tokenizer.get_special_tokens())}")
    
    # BBPEçš„ç‰¹ç‚¹æ˜¯èƒ½å¤Ÿå¤„ç†ä»»æ„Unicodeå­—ç¬¦
    print("\n8. BBPEç‰¹ç‚¹æµ‹è¯•:")
    special_chars = "ğŸ˜€ğŸğŸŒŸÎ±Î²Î³Î´Îµæ¼¢å­—ğˆ"
    print(f"ç‰¹æ®Šå­—ç¬¦: {special_chars}")
    special_tokens = tokenizer.encode(special_chars)
    print(f"ç¼–ç ç»“æœ: {special_tokens}")
    decoded_special = tokenizer.decode(special_tokens)
    print(f"è§£ç ç»“æœ: {decoded_special}")
    if special_chars == decoded_special:
        print("âœ“ ç‰¹æ®Šå­—ç¬¦ç¼–ç è§£ç ä¸€è‡´")
    else:
        print("âœ— ç‰¹æ®Šå­—ç¬¦ç¼–ç è§£ç ä¸ä¸€è‡´")
    
    print("\nç¤ºä¾‹å®Œæˆ!")


if __name__ == "__main__":
    main()