#!/usr/bin/env python3
"""
BBPE (Byte-Level BPE) åˆ†è¯å™¨è®­ç»ƒç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ–‡æœ¬æ•°æ®è®­ç»ƒä¸€ä¸ªæ–°çš„BBPEåˆ†è¯å™¨ã€‚
BBPEåœ¨å­—èŠ‚çº§åˆ«ä¸Šè¿›è¡Œæ“ä½œï¼Œå¯ä»¥å¤„ç†ä»»æ„Unicodeå­—ç¬¦ï¼ŒåŒ…æ‹¬è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ã€‚
"""

import sys
import os
import tempfile
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    import zero_tokenizer
except ImportError:
    print("é”™è¯¯: æ— æ³•å¯¼å…¥zero_tokenizeråº“ã€‚è¯·ç¡®ä¿å·²å®‰è£…Pythonç»‘å®šã€‚")
    print("å¯ä»¥å°è¯•è¿è¡Œ: pip install -e .")
    sys.exit(1)


def generate_sample_text():
    """ç”ŸæˆåŒ…å«å„ç§Unicodeå­—ç¬¦çš„ç¤ºä¾‹è®­ç»ƒæ–‡æœ¬"""
    chinese_sentences = [
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé‡è¦æ–¹å‘ã€‚",
        "åˆ†è¯æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„åŸºç¡€ä»»åŠ¡ä¹‹ä¸€ã€‚",
        "BBPEç®—æ³•æ˜¯BPEçš„å­—èŠ‚çº§åˆ«å˜ä½“ã€‚",
        "æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡çš„æ–‡æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒã€‚",
        "Transformeræ¶æ„åœ¨NLPé¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚",
        "é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¦‚BERTå’ŒGPTæ”¹å˜äº†NLPçš„ç ”ç©¶èŒƒå¼ã€‚",
        "è¯åµŒå…¥æŠ€æœ¯èƒ½å¤Ÿå°†è¯è¯­æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´ã€‚",
        "æ³¨æ„åŠ›æœºåˆ¶æ˜¯Transformeræ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶ã€‚",
        "åºåˆ—åˆ°åºåˆ—æ¨¡å‹é€‚ç”¨äºæœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚",
        "è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚"
    ]
    
    english_sentences = [
        "Natural language processing is an important field in artificial intelligence.",
        "Tokenization is one of the fundamental tasks in NLP.",
        "BBPE algorithm is a byte-level variant of BPE.",
        "Deep learning models require large amounts of text data for training.",
        "The Transformer architecture has achieved great success in the NLP field.",
        "Pre-trained language models like BERT and GPT have changed the NLP research paradigm.",
        "Word embedding techniques can map words to high-dimensional vector spaces.",
        "The attention mechanism is the core component of the Transformer model.",
        "Sequence-to-sequence models are suitable for tasks like machine translation.",
        "Language models can predict the probability distribution of the next word."
    ]
    
    # æ·»åŠ åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„å¥å­
    special_char_sentences = [
        "è¡¨æƒ…ç¬¦å·: ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ˜‚ğŸ¤£â˜ºï¸ğŸ˜ŠğŸ˜‡",
        "åŠ¨ç‰©è¡¨æƒ…: ğŸ¶ğŸ±ğŸ­ğŸ¹ğŸ°ğŸ¦ŠğŸ»ğŸ¼ğŸ¨ğŸ¯",
        "é£Ÿç‰©è¡¨æƒ…: ğŸğŸŠğŸ‹ğŸŒğŸ‰ğŸ‡ğŸ“ğŸ«ğŸˆğŸ’",
        "å¸Œè…Šå­—æ¯: Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰",
        "æ•°å­¦ç¬¦å·: âˆ‘âˆâˆ«âˆ†âˆ‡âˆ‚âˆÂ±Ã—Ã·â‰ â‰¤â‰¥â‰ˆâˆâˆˆâˆ‰âŠ‚âŠƒâŠ†âŠ‡",
        "ç‰¹æ®ŠUnicode: ğˆğ‰ğŠğ‹ğŒğğğğğ‘"
    ]
    
    # æ··åˆæ‰€æœ‰å¥å­ï¼Œå¹¶æ·»åŠ ä¸€äº›éšæœºå˜åŒ–
    all_sentences = chinese_sentences + english_sentences + special_char_sentences
    random.shuffle(all_sentences)
    
    return all_sentences


def main():
    print("BBPEåˆ†è¯å™¨è®­ç»ƒç¤ºä¾‹")
    print("=" * 50)
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    print("\n1. ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    sentences = generate_sample_text()
    print(f"âœ“ ç”Ÿæˆäº† {len(sentences)} ä¸ªè®­ç»ƒå¥å­")
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜è®­ç»ƒæ•°æ®
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
        for sentence in sentences:
            f.write(sentence + '\n')
        training_file = f.name
    
    try:
        # åˆ›å»ºç©ºçš„BBPEåˆ†è¯å™¨
        print("\n2. åˆ›å»ºç©ºçš„BBPEåˆ†è¯å™¨...")
        tokenizer = zero_tokenizer.bbpe()
        print("âœ“ BBPEåˆ†è¯å™¨åˆ›å»ºæˆåŠŸ")
        
        # è®¾ç½®è®­ç»ƒå‚æ•°
        vocab_size = 1000  # è¯æ±‡è¡¨å¤§å°
        special_tokens = ["<pad>", "<unk>", "<s>", "</s>"]  # ç‰¹æ®Štoken
        
        print(f"\n3. å¼€å§‹è®­ç»ƒåˆ†è¯å™¨...")
        print(f"   è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"   ç‰¹æ®Štokens: {special_tokens}")
        print(f"   è®­ç»ƒæ–‡ä»¶: {training_file}")
        
        # è®­ç»ƒåˆ†è¯å™¨
        tokenizer.train(
            files=[training_file],
            vocab_size=vocab_size,
            special_tokens=special_tokens
        )
        
        print("âœ“ åˆ†è¯å™¨è®­ç»ƒå®Œæˆ")
        
        # æµ‹è¯•è®­ç»ƒåçš„åˆ†è¯å™¨
        print("\n4. æµ‹è¯•è®­ç»ƒåçš„åˆ†è¯å™¨...")
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•BBPEåˆ†è¯å™¨çš„å¥å­ã€‚This is a test sentence. ğŸ˜ŠğŸš€"
        print(f"æµ‹è¯•æ–‡æœ¬: {test_text}")
        
        # ç¼–ç 
        tokens = tokenizer.encode(test_text)
        print(f"ç¼–ç ç»“æœ: {tokens}")
        
        # è§£ç 
        decoded_text = tokenizer.decode(tokens)
        print(f"è§£ç ç»“æœ: {decoded_text}")
        
        # éªŒè¯ä¸€è‡´æ€§
        if test_text == decoded_text:
            print("âœ“ ç¼–ç è§£ç ä¸€è‡´")
        else:
            print("âœ— ç¼–ç è§£ç ä¸ä¸€è‡´")
        
        # æ˜¾ç¤ºä¸€äº›è¯æ±‡è¡¨å†…å®¹
        print("\n5. è¯æ±‡è¡¨ç¤ºä¾‹ (å‰20ä¸ª):")
        for i in range(min(20, tokenizer.get_vocab_size())):
            token = tokenizer.id_to_token(i)
            print(f"  ID {i}: '{token}'")
        
        # BBPEçš„ç‰¹ç‚¹æ˜¯èƒ½å¤Ÿå¤„ç†ä»»æ„Unicodeå­—ç¬¦
        print("\n6. BBPEç‰¹ç‚¹æµ‹è¯•:")
        special_chars = "ğŸ˜€ğŸğŸŒŸÎ±Î²Î³Î´Îµæ¼¢å­—ğˆ"
        print(f"ç‰¹æ®Šå­—ç¬¦: {special_chars}")
        special_tokens = tokenizer.encode(special_chars)
        print(f"ç¼–ç ç»“æœ: {special_tokens}")
        decoded_special = tokenizer.decode(special_tokens)
        print(f"è§£ç ç»“æœ: {decoded_special}")
        if special_chars == decoded_special:
            print("âœ“ ç‰¹æ®Šå­—ç¬¦ç¼–ç è§£ç ä¸€è‡´")
        else:
            print("âœ— ç‰¹æ®Šå­—ç¬¦ç¼–ç è§£ç ä¸ä¸€è‡´")
        
        # ä¿å­˜è®­ç»ƒå¥½çš„åˆ†è¯å™¨
        model_file = "bbpe_tokenizer.json"
        print(f"\n7. ä¿å­˜è®­ç»ƒå¥½çš„åˆ†è¯å™¨åˆ° {model_file}...")
        tokenizer.save(model_file)
        print("âœ“ åˆ†è¯å™¨ä¿å­˜æˆåŠŸ")
        
        # åŠ è½½ä¿å­˜çš„åˆ†è¯å™¨
        print("\n8. åŠ è½½ä¿å­˜çš„åˆ†è¯å™¨...")
        loaded_tokenizer = zero_tokenizer.bbpe()
        loaded_tokenizer.load(model_file)
        print("âœ“ åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # éªŒè¯åŠ è½½çš„åˆ†è¯å™¨
        loaded_tokens = loaded_tokenizer.encode(test_text)
        if tokens == loaded_tokens:
            print("âœ“ åŠ è½½çš„åˆ†è¯å™¨ä¸åŸå§‹åˆ†è¯å™¨ä¸€è‡´")
        else:
            print("âœ— åŠ è½½çš„åˆ†è¯å™¨ä¸åŸå§‹åˆ†è¯å™¨ä¸ä¸€è‡´")
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(training_file)
        if os.path.exists(model_file):
            os.unlink(model_file)
    
    print("\nç¤ºä¾‹å®Œæˆ!")


if __name__ == "__main__":
    main()