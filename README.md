# Zero Tokenizer

ä¸€ä¸ªé«˜æ€§èƒ½çš„LLMæ¨¡å‹åˆ†è¯å™¨åº“ï¼Œä½¿ç”¨Rustè¯­è¨€å®ç°ï¼Œæ”¯æŒå¤šç§åˆ†è¯ç®—æ³•ï¼ŒåŒ…æ‹¬BPEã€BBPEã€Unigramå’ŒWordPieceã€‚

## ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½**ï¼šä½¿ç”¨Rustå®ç°ï¼Œæä¾›æè‡´çš„æ€§èƒ½
- ğŸ“š **å¤šç®—æ³•æ”¯æŒ**ï¼šæ”¯æŒBPEã€BBPEã€Unigramå’ŒWordPieceå››ç§ä¸»æµåˆ†è¯ç®—æ³•
- ğŸŒ **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ”¯æŒå¤šç§è¯­è¨€çš„æ–‡æœ¬å¤„ç†ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ç­‰
- ğŸ”§ **çµæ´»é…ç½®**ï¼šæ”¯æŒè‡ªå®šä¹‰è¯æ±‡è¡¨å¤§å°ã€ç‰¹æ®Štokenå’Œæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
- ğŸ **Pythonç»‘å®š**ï¼šæä¾›Pythonæ¥å£ï¼Œæ–¹ä¾¿åœ¨Pythoné¡¹ç›®ä¸­ä½¿ç”¨
- ğŸ“Š **å¹¶è¡Œå¤„ç†**ï¼šä½¿ç”¨Rayonåº“å®ç°å¹¶è¡Œå¤„ç†ï¼Œæé«˜è®­ç»ƒæ•ˆç‡
- ğŸ”„ **æµå¼è®­ç»ƒ**ï¼šæ”¯æŒä»è¿­ä»£å™¨æµå¼è®­ç»ƒï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†

## å®‰è£…

### Rust

åœ¨æ‚¨çš„`Cargo.toml`ä¸­æ·»åŠ ï¼š

```toml
[dependencies]
zero-tokenizer = "0.1.0"
```

### Python

```bash
pip install zero-tokenizer
```

## å¿«é€Ÿå¼€å§‹

### BPEåˆ†è¯å™¨

```python
from zero_tokenizer import BPETokenizer

# åˆ›å»ºåˆ†è¯å™¨
tokenizer = BPETokenizer()

# è®­ç»ƒåˆ†è¯å™¨
tokenizer.train(
    files=["path/to/your/data.txt"],
    vocab_size=30000,
    special_tokens=["<unk>", "<s>", "</s>"]
)

# ç¼–ç æ–‡æœ¬
text = "Hello, world!"
tokens = tokenizer.encode(text)
print(f"Tokens: {tokens}")

# è§£ç tokens
decoded_text = tokenizer.decode(tokens)
print(f"Decoded: {decoded_text}")
```

### BBPEåˆ†è¯å™¨

```python
from zero_tokenizer import BBPETokenizer

# åˆ›å»ºåˆ†è¯å™¨
tokenizer = BBPETokenizer()

# è®­ç»ƒåˆ†è¯å™¨
tokenizer.train(
    files=["path/to/your/data.txt"],
    vocab_size=50000,
    special_tokens=["<unk>", "<s>", "</s>"]
)

# ç¼–ç æ–‡æœ¬
text = "ä½ å¥½ï¼Œä¸–ç•Œï¼"
tokens = tokenizer.encode(text)
print(f"Tokens: {tokens}")

# è§£ç tokens
decoded_text = tokenizer.decode(tokens)
print(f"Decoded: {decoded_text}")
```

### Unigramåˆ†è¯å™¨

```python
from zero_tokenizer import UnigramTokenizer

# åˆ›å»ºåˆ†è¯å™¨
tokenizer = UnigramTokenizer()

# è®­ç»ƒåˆ†è¯å™¨
tokenizer.train(
    files=["path/to/your/data.txt"],
    vocab_size=30000,
    special_tokens=["<unk>", "<s>", "</s>"]
)

# ç¼–ç æ–‡æœ¬
text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
tokens = tokenizer.encode(text)
print(f"Tokens: {tokens}")

# è§£ç tokens
decoded_text = tokenizer.decode(tokens)
print(f"Decoded: {decoded_text}")
```

### WordPieceåˆ†è¯å™¨

```python
from zero_tokenizer import WordPieceTokenizer

# åˆ›å»ºåˆ†è¯å™¨
tokenizer = WordPieceTokenizer()

# è®­ç»ƒåˆ†è¯å™¨
tokenizer.train(
    files=["path/to/your/data.txt"],
    vocab_size=30000,
    special_tokens=["<unk>", "<s>", "</s>"]
)

# ç¼–ç æ–‡æœ¬
text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
tokens = tokenizer.encode(text)
print(f"Tokens: {tokens}")

# è§£ç tokens
decoded_text = tokenizer.decode(tokens)
print(f"Decoded: {decoded_text}")
```

## ç®—æ³•ä»‹ç»

### BPE (Byte Pair Encoding)

BPEæ˜¯ä¸€ç§åŸºäºé¢‘ç‡çš„å­è¯åˆ†è¯ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£åˆå¹¶æœ€é¢‘ç¹å‡ºç°çš„å­—ç¬¦å¯¹æ¥æ„å»ºè¯æ±‡è¡¨ã€‚å®ƒç®€å•é«˜æ•ˆï¼Œæ˜“äºæ§åˆ¶è¯æ±‡è¡¨å¤§å°ï¼Œè¢«å¹¿æ³›åº”ç”¨äºGPTç³»åˆ—æ¨¡å‹ä¸­ã€‚

[è¯¦ç»†æ–‡æ¡£](docs/bpe.md)

### BBPE (Byte-Level BPE)

BBPEæ˜¯BPEçš„å­—èŠ‚çº§å˜ä½“ï¼Œç›´æ¥åœ¨å­—èŠ‚çº§åˆ«è¿›è¡Œæ“ä½œï¼Œèƒ½å¤Ÿå¤„ç†ä»»æ„Unicodeå­—ç¬¦ï¼ŒåŒ…æ‹¬è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ã€‚

[è¯¦ç»†æ–‡æ¡£](docs/bbpe.md)

### Unigram

Unigramæ˜¯ä¸€ç§åŸºäºæ¦‚ç‡çš„è¯­è¨€æ¨¡å‹ï¼Œä»ä¸€ä¸ªå¤§çš„åˆå§‹è¯æ±‡è¡¨å¼€å§‹ï¼Œé€æ­¥ç§»é™¤ä¸é‡è¦çš„å­è¯ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯æ±‡è¡¨å¤§å°ã€‚å®ƒæ”¯æŒå¤šç§åˆ†è¯ç­–ç•¥ï¼Œçµæ´»æ€§é«˜ã€‚

[è¯¦ç»†æ–‡æ¡£](docs/unigram.md)

### WordPiece

WordPieceæ˜¯ä¸€ç§åŸºäºæ¦‚ç‡çš„å­è¯åˆ†è¯ç®—æ³•ï¼Œé€‰æ‹©èƒ½å¤Ÿæœ€å¤§åŒ–è¯­è¨€æ¨¡å‹ä¼¼ç„¶çš„å­è¯åˆå¹¶ã€‚å®ƒè¢«å¹¿æ³›åº”ç”¨äºBERTç­‰é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­ã€‚

[è¯¦ç»†æ–‡æ¡£](docs/wordpiece.md)

## ç¤ºä¾‹

åœ¨`examples`ç›®å½•ä¸­æä¾›äº†æ¯ç§ç®—æ³•çš„è¯¦ç»†ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š

- `examples/bpe/`: BPEç®—æ³•ç¤ºä¾‹
  - `usage_example.py`: BPEåˆ†è¯å™¨ä½¿ç”¨ç¤ºä¾‹
  - `training_example.py`: BPEåˆ†è¯å™¨è®­ç»ƒç¤ºä¾‹
- `examples/bbpe/`: BBPEç®—æ³•ç¤ºä¾‹
  - `usage_example.py`: BBPEåˆ†è¯å™¨ä½¿ç”¨ç¤ºä¾‹
  - `training_example.py`: BBPEåˆ†è¯å™¨è®­ç»ƒç¤ºä¾‹
- `examples/unigram/`: Unigramç®—æ³•ç¤ºä¾‹
  - `usage_example.py`: Unigramåˆ†è¯å™¨ä½¿ç”¨ç¤ºä¾‹
  - `training_example.py`: Unigramåˆ†è¯å™¨è®­ç»ƒç¤ºä¾‹
- `examples/wordpiece/`: WordPieceç®—æ³•ç¤ºä¾‹
  - `usage_example.py`: WordPieceåˆ†è¯å™¨ä½¿ç”¨ç¤ºä¾‹
  - `training_example.py`: WordPieceåˆ†è¯å™¨è®­ç»ƒç¤ºä¾‹

## æ€§èƒ½

| ç®—æ³• | è®­ç»ƒé€Ÿåº¦ | æ¨ç†é€Ÿåº¦ | è¯æ±‡è¡¨å¤§å° | æ”¯æŒè¯­è¨€ |
|------|----------|----------|------------|----------|
| BPE | å¿« | å¿« | å¯æ§ | å¤šè¯­è¨€ |
| BBPE | ä¸­ç­‰ | å¿« | è¾ƒå¤§ | å¤šè¯­è¨€ |
| Unigram | æ…¢ | ä¸­ç­‰ | å¯æ§ | å¤šè¯­è¨€ |
| WordPiece | æ…¢ | å¿« | å¯æ§ | å¤šè¯­è¨€ |

## å¼€å‘

### æ„å»ºé¡¹ç›®

```bash
# æ„å»ºRuståº“
cargo build --release

# æ„å»ºPythonç»‘å®š
maturin develop
```

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡ŒRustæµ‹è¯•
cargo test

# è¿è¡ŒPythonæµ‹è¯•
python -m pytest tests/
```

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š
```bash
1. Fork æœ¬ä»“åº“
2. åˆ›å»ºæ‚¨çš„ç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/xxx`)
3. æäº¤æ‚¨çš„æ›´æ”¹ (`git commit -m 'Add some xxx feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/xxx`)
5. æ‰“å¼€ä¸€ä¸ª Pull Request
```
## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨åŒé‡è®¸å¯è¯ - æ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨ Apache 2.0 è®¸å¯è¯æˆ– MIT è®¸å¯è¯ã€‚è¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹ [LICENSE](LICENSE.md) æ–‡ä»¶ã€‚

## è‡´è°¢
æ„Ÿè°¢ [@code-your-own-llm](https://github.com/datawhalechina/code-your-own-llm)ã€[@nanochat](https://github.com/karpathy/nanochat) æä¾›çµæ„Ÿå’Œå‚è€ƒ, ä»¥åŠæ„Ÿè°¢CME295è¯¾ç¨‹çš„æ•™æã€PPTç­‰èµ„æº,æ¨èè´­ä¹°ç”µå­ä¹¦[Super Study Guide: Transformer ä¸å¤§è¯­è¨€æ¨¡å‹](https://leanpub.com/transformer-da-yuyan-moxing/)ã€‚


## æ›´æ–°æ—¥å¿—

### v0.1.0 (2025-11-11)

- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒBPEã€BBPEã€Unigramå’ŒWordPieceå››ç§ç®—æ³•
- æä¾›Pythonç»‘å®š
- æ·»åŠ è¯¦ç»†æ–‡æ¡£å’Œç¤ºä¾‹